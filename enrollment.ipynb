{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrollment\n",
    "\n",
    "This notebook collects enrollment data on courses of interest from Berkeley's [course catalog](http://classes.berkeley.edu/) and then performs some analysis of that data.\n",
    "\n",
    "## Collecting the data\n",
    "If anyone hears of an API for the course catalog, we should use that. Otherwise, we scrape. Each course has its own page on the catalog. Take the URL of the page of each course that you're interested in (e.g. `http://classes.berkeley.edu/content/2017-spring-info-290-006-lec-006`) and put it on its own line in a file called _urls.txt_. The notebook will scrape all the URLs, extract enrollment information and save it.\n",
    "\n",
    "All the data relevant to enrollments (and other interesting attributes of the course) are stored in four JSON objects called _data-json_, _data-enrollment_, _data-term-details_ and _data-node_ in the HTML of each page. So once we've found those four, it's just a matter of selecting the information that you want from them. Unfortunately, the JSON objects for enrollment information don't have the same hierarchy or names for every course and I can't find any documentation on how it's formatted. To get around this, I've just dealt with the range of hierarchies and names that I've come across so far, by branching depending on what keys are in the JSON.\n",
    "\n",
    "If something breaks in the data collection, it's likely because I've made assumptions about how the structure of the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URLS_FNAME = 'urls.txt'\n",
    "OUT_FNAME = 'enrollments.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These functions are where we look through the JSON objects for the data we care about.\n",
    "\n",
    "def get_course_title(data_json):\n",
    "    return data_json['class']['course']['title']\n",
    "\n",
    "def get_course_number(data_json):\n",
    "    return data_json['id']\n",
    "\n",
    "def remove_enrollment_status_level(d):\n",
    "    \"\"\"\n",
    "    The data-enrollment JSON sometimes has different hierarchies and names.\n",
    "    This function is just to help overcome that. This function is only used \n",
    "    in `get_enrollment_dict` below.\n",
    "    \"\"\"\n",
    "    if 'enrollmentStatus' in d:\n",
    "        return d['enrollmentStatus']\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "def get_enrollment_dict(data_enrollment):\n",
    "    \"\"\"\n",
    "    The data-enrollment JSON sometimes has different hierarchies and names.\n",
    "    This function is just to help overcome that. Returns all relevant enrollment \n",
    "    data in a dict.\n",
    "    \"\"\"\n",
    "    if 'available' in data_enrollment:\n",
    "        return remove_enrollment_status_level(data_enrollment['available'])\n",
    "    else:\n",
    "        return remove_enrollment_status_level(data_enrollment)\n",
    "    \n",
    "def get_enrolled_count(data_enrollment):\n",
    "    enrollment_dict = get_enrollment_dict(data_enrollment)\n",
    "    return enrollment_dict['enrolledCount']\n",
    "\n",
    "def get_max_enrollment(data_enrollment):\n",
    "    enrollment_dict = get_enrollment_dict(data_enrollment)\n",
    "    return enrollment_dict['maxEnroll']\n",
    "\n",
    "def get_waitlist_count(data_enrollment):\n",
    "    enrollment_dict = get_enrollment_dict(data_enrollment)\n",
    "    return enrollment_dict['waitlistedCount']\n",
    "\n",
    "def get_max_waitlist(data_enrollment):\n",
    "    enrollment_dict = get_enrollment_dict(data_enrollment)\n",
    "    return enrollment_dict['maxWaitlist']\n",
    "\n",
    "def get_enrollment_status(data_enrollment):\n",
    "    enrollment_dict = get_enrollment_dict(data_enrollment)\n",
    "    return enrollment_dict['status']['description']\n",
    "\n",
    "def parse_session(session_string):\n",
    "    year, semester = session_string.split()\n",
    "    return int(year), semester\n",
    "    \n",
    "def get_session_date(data_term_details):\n",
    "    return parse_session(data_term_details['sessionDescription'])\n",
    "\n",
    "def get_department(data_json):\n",
    "    return data_json['academicOrganization']['description']\n",
    "\n",
    "def get_course_level(data_json):\n",
    "    return data_json['course']['academicCareer']['description']\n",
    "\n",
    "def get_academic_group(data_json):\n",
    "    return data_json['academicGroup']['formalDescription']\n",
    "\n",
    "def get_start_date(data_json):\n",
    "    return data_json['startDate']\n",
    "\n",
    "def get_end_date(data_json):\n",
    "    return data_json['endDate']\n",
    "\n",
    "def get_location(data_json):\n",
    "    return data_json['meetings'][0]['location']['description']\n",
    "\n",
    "def get_number_of_units(data_json):\n",
    "    return data_json['class']['allowedUnits']['forAcademicProgress']\n",
    "\n",
    "def make_short_course_name(url):\n",
    "    \"\"\"Return names like 'Stat 134' from `url`.\n",
    "    \n",
    "    These short names are often more useful than the full title.\n",
    "    Unfortunately, because Data 8 and Data 100 are listed in the \n",
    "    Stats/CS departments, they won't appear with their usual names.\"\"\"\n",
    "    pattern = r'\\d{4}-(?:fall|spring|summer)-(.*?)-(.*?)-.*'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        dept, num = match.groups()\n",
    "        return dept + ' ' + num\n",
    "    return \"Couldn't make short course name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data_for_one_course(url):\n",
    "    \"\"\"Collect desired enrollment data for course listed at `url`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        The URL of the course you're interested in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Selected data points of that course.\n",
    "\n",
    "    \"\"\"\n",
    "    # Scrape the data\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Find the four JSON objects\n",
    "    data_div = soup.find_all('div', 'handlebarData theme_is_cc_berkeley')[0]\n",
    "    data_enrollment = json.loads(data_div['data-enrollment'])\n",
    "    data_json = json.loads(data_div['data-json'])\n",
    "    data_term_details = json.loads(data_div['data-term-details'])\n",
    "    data_node = json.loads(data_div['data-node'])\n",
    "    \n",
    "    # Select the data you want from those four JSON objects.\n",
    "    # The following lines are grouped by logically related data,\n",
    "    # not which JSON object they come from\n",
    "    \n",
    "    # BASIC INFO ON COURSE\n",
    "    title = get_course_title(data_json)\n",
    "    course_number = get_course_number(data_json)\n",
    "    year, semester = get_session_date(data_term_details)\n",
    "    department = get_department(data_json)\n",
    "    \n",
    "    # ENROLLMENT DATA\n",
    "    enrolled_count = get_enrolled_count(data_enrollment)\n",
    "    max_enrollment = get_max_enrollment(data_enrollment)\n",
    "    waitlist_count = get_waitlist_count(data_enrollment)\n",
    "    max_waitlist = get_max_waitlist(data_enrollment)\n",
    "    enrollment_status = get_enrollment_status(data_enrollment)\n",
    "    open_seats = max_enrollment - enrolled_count # Not in the data received, but we can calculate it\n",
    "    \n",
    "    # OTHER INFO ON COURSE - YOU MAY NOT CARE ABOUT ANY OF THIS\n",
    "    # Currently, getting the level and location are known to have bugs\n",
    "    # for course urls we've tried. This is because of assumptions on\n",
    "    # the structure of the JSON. Because it's non-essential, I've just\n",
    "    # commented these out.\n",
    "    #level = get_course_level(data_json)\n",
    "    #academic_group = get_academic_group(data_json)\n",
    "    #start_date = get_start_date(data_json)\n",
    "    #end_date = get_end_date(data_json)\n",
    "    #location = get_location(data_json)\n",
    "    #number_of_units = get_number_of_units(data_json)\n",
    "    \n",
    "    # THE TIME THIS DATA WAS COLLECTED\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    # Make short course name\n",
    "    short_name = make_short_course_name(url)\n",
    "    \n",
    "    # Build the result as a dictionary so that when we turn it into a df later,\n",
    "    # we don't have to specify in two places what the column names are.\n",
    "    result = {'title': title,\n",
    "              'course_number': course_number,\n",
    "              'short_name': short_name,\n",
    "              'year': year,\n",
    "              'semester': semester,\n",
    "              'department': department,\n",
    "              'enrolled_count': enrolled_count,\n",
    "              'max_enrollment': max_enrollment,\n",
    "              'waitlist_count': waitlist_count,\n",
    "              'max_waitlist': max_waitlist,\n",
    "              'enrollment_status': enrollment_status,\n",
    "              'open_seats': open_seats,\n",
    "              'collected': now}\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_data_for_all_courses(urls):\n",
    "    \"\"\"Collect desired enrollment data for all courses listed in `urls`.\n",
    "    \n",
    "    This function exists to handle any unexpected errors with a single course.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls: list(str)\n",
    "        The URLs of the courses you're interested in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            result = extract_data_for_one_course(url)\n",
    "            results.append(result)\n",
    "        except:\n",
    "            print('Something went wrong with {}'.format(url))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "Here we read in the URLs from _url.txt_, extract all the data we care about, put it in a pandas DataFrame and save the data to a csv file, appending to an existing file if it already exists. If it does already exist, then we don't want to write the header row again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-compsci-c8-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-stat-c100-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-stat-140-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-eps-88-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-civeng-88-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-compsci-88-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-demog-88-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-history-88-001-lab-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-physics-88-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-stat-88-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-ugba-96-002-lec-002\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-ugba-96-003-lec-003\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-africam-134-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-bioeng-100-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-history-c182c-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-isf-100j-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-compsci-194-031-lec-031\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-demog-110-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-eneres-190c-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-espm-157-001-lab-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-indeng-135-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-indeng-142-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-info-159-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-physics-77-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-physics-151-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-stat-133-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-stat-159-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-compsci-10-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-compsci-61A-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-compsci-61B-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-math-54-001-lec-001\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-math-54-002-lec-002\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-math-54-003-lec-003\n",
      "Something went wrong with http://classes.berkeley.edu/content/2018-fall-compsci-188-001-lec-001\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['title' 'course_number' 'short_name' 'department' 'semester' 'year'\\n 'enrolled_count' 'max_enrollment' 'open_seats' 'waitlist_count'\\n 'max_waitlist' 'enrollment_status' 'collected'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f57c66558030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m cols = ['title', 'course_number', 'short_name', 'department', 'semester', 'year','enrolled_count', 'max_enrollment', \n\u001b[1;32m      6\u001b[0m         'open_seats', 'waitlist_count', 'max_waitlist', 'enrollment_status', 'collected']\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mneed_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_FNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_FNAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1964\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1965\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2005\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['title' 'course_number' 'short_name' 'department' 'semester' 'year'\\n 'enrolled_count' 'max_enrollment' 'open_seats' 'waitlist_count'\\n 'max_waitlist' 'enrollment_status' 'collected'] not in index\""
     ]
    }
   ],
   "source": [
    "with open('urls.txt') as f:\n",
    "    urls = f.read().splitlines()\n",
    "data = extract_data_for_all_courses(urls)\n",
    "df = pd.DataFrame(data)\n",
    "cols = ['title', 'course_number', 'short_name', 'department', 'semester', 'year','enrolled_count', 'max_enrollment', \n",
    "        'open_seats', 'waitlist_count', 'max_waitlist', 'enrollment_status', 'collected']\n",
    "df = df[cols]\n",
    "need_header = not os.path.exists(OUT_FNAME)\n",
    "df.to_csv(OUT_FNAME, index=False, mode='a+', header=need_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Every time we run the cell above, we collect the current data for the courses, but we don't have the historical data. So we read in the whole csv which has been accumulating all the data every time we scrape it. Then we do some basic analysis of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(OUT_FNAME)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='enrolled_count', inplace=True, ascending=False)\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.barplot(x='enrolled_count', y='title', data=df, color='maroon', orient='h', ci=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "The script above outputs a single file that stacks all collection dates on top of each other. Users may prefer a single file for each date of collection. The code below filters the dataframe to contain only the data that was collected on the specified date of interest. Just change the year, month and day values in `date_of_interest` and the final line subsets the whole dataframe based on this date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_of_interest = datetime.date(datetime(2018, 6, 4))\n",
    "collected_on_date_of_interest = pd.to_datetime(df['collected']).dt.date == date_of_interest\n",
    "df[collected_on_date_of_interest]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
